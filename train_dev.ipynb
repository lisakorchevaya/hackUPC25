{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eca43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"image.cmap\"] = \"viridis\"\n",
    "plt.rcParams['font.family'] = 'Liberation Sans'\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(trial, X_t):\n",
    "    # Preprocessing hyperparameters\n",
    "    # Feature Selection / Engineering\n",
    "    feature_selector = trial.suggest_categorical('feature_selector', ['none', 'lasso', 'elasticnet'])\n",
    "    if feature_selector == 'none':\n",
    "        selector = None\n",
    "    elif feature_selector == 'lasso':\n",
    "        selector = SelectFromModel(\n",
    "            Lasso(alpha=trial.suggest_float('lasso_alpha', 1e-4, 1.0, log=True)),\n",
    "                max_features=trial.suggest_int('lasso_max_features', 50, 300),\n",
    "                threshold='mean'\n",
    "        )\n",
    "\n",
    "    elif feature_selector == 'elasticnet':\n",
    "        selector = SelectFromModel(\n",
    "            ElasticNet(\n",
    "        alpha=trial.suggest_float('alpha', 1e-4, 1.0, log=True),\n",
    "        l1_ratio=trial.suggest_float('l1_ratio', 0.1, 1.0)),\n",
    "        threshold='mean'\n",
    "        )\n",
    "\n",
    "    # Scaler\n",
    "    scaler_name = trial.suggest_categorical('scaler', ['standard', 'minmax', 'robust'])\n",
    "    scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'minmax': MinMaxScaler(),\n",
    "    'robust': RobustScaler()\n",
    "    }\n",
    "    scaler = scalers[scaler_name]\n",
    "\n",
    "    # Suggest model type\n",
    "    model_name = trial.suggest_categorical('model', ['RandomForest', 'DecisionTree', 'XGBoost', 'HistGradientBosting', 'LightGBM'])\n",
    "    \n",
    "\n",
    "    if model_name == 'RandomForest':\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=trial.suggest_int('rf_n_estimators', 10, 300),\n",
    "            criterion=trial.suggest_categorical('rf_criterion', ['gini', 'entropy']),\n",
    "            max_depth=trial.suggest_int('rf_max_depth', 3, 30),\n",
    "            min_samples_split=trial.suggest_int('rf_min_samples_split', 2, 10),\n",
    "            min_samples_leaf=trial.suggest_int('rf_min_samples_leaf', 1, 35),\n",
    "            bootstrap=trial.suggest_categorical('rf_bootstrap', [True, False]),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_name == 'DecisionTree':\n",
    "        model = DecisionTreeRegressor(\n",
    "            criterion=trial.suggest_categorical('dt_criterion', ['gini', 'entropy']),\n",
    "            splitter=trial.suggest_categorical('dt_splitter', ['best', 'random']),\n",
    "            max_depth=trial.suggest_int('dt_max_depth', 3, 30),\n",
    "            min_samples_split=trial.suggest_int('dt_min_samples_split', 2, 10),\n",
    "            min_samples_leaf=trial.suggest_int('dt_min_samples_leaf', 1, 35),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_name == 'XGBoost':\n",
    "        model = XGBRegressor(\n",
    "            learning_rate=trial.suggest_float('xgb_learning_rate', 0.01, 1.0, log=True),\n",
    "            n_estimators=trial.suggest_int('xgb_n_estimators', 20, 200),\n",
    "            max_depth=trial.suggest_int('xgb_max_depth', 3, 30),\n",
    "            booster=trial.suggest_categorical('xgb_booster', ['gbtree', 'gblinear', 'dart']),\n",
    "            alpha=trial.suggest_float('xgb_alpha', 0.0, 1.0),\n",
    "            lambda_=trial.suggest_float('xgb_lambda', 0.0, 1.0),\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    elif model_name == 'HistGradientBosting':\n",
    "        model = HistGradientBoostingRegressor(\n",
    "            warm_start=True,\n",
    "            learning_rate=trial.suggest_float('hgb_learning_rate', 0.01, 1.0, log=True),\n",
    "            max_iter=trial.suggest_int('hgb_max_iter', 100, 1000),\n",
    "            max_depth=trial.suggest_int('hgb_max_depth', 3, 40),\n",
    "            max_bins=trial.suggest_int('hgb_max_bins', 128, 256)\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_name == 'LightGBM':\n",
    "        model = LGBMRegressor(\n",
    "            verbosity=-1,\n",
    "            learning_rate=trial.suggest_float('lgb_learning_rate', 0.01, 1.0, log=True),\n",
    "            n_estimators=trial.suggest_int('lgb_n_estimators', 20, 500),\n",
    "            num_leaves=trial.suggest_int('lgb_num_leaves', 20, 300),\n",
    "            max_depth=trial.suggest_int('lgb_max_depth', -1, 15)\n",
    "            feature_fraction=trial.suggest_float('lgb_feature_fraction', 0.4, 1.0),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # Assemble the pipeline\n",
    "    steps = [\n",
    "        ('scaler', scaler),\n",
    "    ]\n",
    "    if feature_selector != 'none':\n",
    "        steps.append(('feature_selector', selector))\n",
    "    steps.append(('model', model))\n",
    "\n",
    "    pipeline = Pipeline(steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop for nested cross validation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna for hyperparameter optimization\n",
    "def objective(trial):\n",
    "    try:\n",
    "        # StratifiedKFold cross-validation\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Initialize lists to store metrics\n",
    "        mean_squared_error_scores = []\n",
    "        mean_absolute_error_scores = []\n",
    "        root_mean_squared_error_scores = []\n",
    "        r2_score_scores = []\n",
    "        \n",
    "        # Initialize the split\n",
    "        for train_idx, test_idx in skf.split(X_train, y_train):\n",
    "            X_t, X_val = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "            y_t, y_val = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "            \n",
    "            # Create pipeline based on trial hyperparameters\n",
    "            pipeline = create_pipe(trial, X_t)\n",
    "            \n",
    "            # Fit the model\n",
    "            pipeline.fit(X_t, y_t)\n",
    "            \n",
    "            # Predict the validation set\n",
    "            y_val_pred = pipeline.predict(X_val)\n",
    "            \n",
    "            # Compute metrics\n",
    "            mse = mean_squared_error(y_val, y_val_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            mean_squared_error_scores.append(mse)\n",
    "            mean_absolute_error_scores.append(mean_absolute_error(y_val, y_val_pred))\n",
    "            root_mean_squared_error_scores.append(rmse)\n",
    "            r2_score_scores.append(r2_score(y_val, y_val_pred))\n",
    "        \n",
    "        # Compute mean of all scores\n",
    "        trial.set_user_attr('MSE', np.mean(mean_squared_error_scores))\n",
    "        trial.set_user_attr('MAE', np.mean(mean_absolute_error_scores))\n",
    "        trial.set_user_attr('RMSE', np.mean(root_mean_squared_error_scores))\n",
    "        trial.set_user_attr('R2', np.mean(r2_score_scores))\n",
    "        \n",
    "        # Return RMSE as the optimization target (lower is better)\n",
    "        return np.mean(root_mean_squared_error_scores)\n",
    "\n",
    "    except ValueError as e:\n",
    "        trial.set_user_attr(\"error\", str(e))\n",
    "        raise optuna.exceptions.TrialPruned()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e42554",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=500, n_jobs=35, show_progress_bar=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alloviz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
